\documentclass[Report.tex]{subfiles}

\begin{document}

\section{Methodology}\label{sec:methodology}
This section describes the methodology for each part of the project, explaining the design decisions in details. The reasoning behind the different subsets of features, how they are extracted from match replays and the process for data collection are also discussed.

\subsection{Mouse movement features}\label{sec:mm-features}
As using the mouse is a key part of controlling the player's hero in the game, it is the first place to investigate what kind of features can be created. The mouse movement features extracted from the Dota 2 match replays followed the approach by Feher et al \cite{mouse-dynamics} on user identification via mouse dynamics. Their methodology was to split the mouse movements into different types of actions, such as mouse movement followed by left click, mouse movement followed by drag etc. The concept of lower and higher level actions formed a hierarchy between atomic actions such as a left click or mouse move and more complex actions made up of multiple atomic actions such as mouse drag, which consists of left click down, mouse movement and left click up actions in order. This approach was adapted to the data available from Dota 2 replays, which was less precise than what could be normally measured as user input and to include keyboard presses where possible. 

There was a significant lack of precision in the data retrieved using the parser on a replay, especially when compared to direct recording of user mouse movements. For example, with live recording of mouse movements, multiple events and features can be extracted from a single mouse click: left click up, left click down, click time (the time between click up and click down events) and distanced travelled during the click. From a replay, the clicks themselves are not registered, only the positions of the cursor and the time the commands are received by the user. This limits the number of features that can be generated compared to directly recording mouse events.

While multiple levels of mouse actions are defined by Feher et al \cite{mouse-dynamics}, only two are defined in this project: \textbf{Level 1 actions} and \textbf{Level 3 actions}.

Four \textbf{level 1 actions} are defined, they are:
\begin{itemize}
\item Mouse movement sequence (MM)
\item Attack command (AC)
\item Move command (MC)
\item Spell cast command (SC)
\end{itemize}
A mouse movement sequence is defined as a sequence of positions of the cursor. Rather than using a fixed interval of time in which the sequence must fit, a threshold $\tau$ is used to end the sequence if no change in cursor position has occurred within the threshold time. This more naturally records a sequence of mouse movements that doesn't break up a sequence of movements to fit within a fixed time interval. A larger threshold increases the average length of MM actions as more mouse positions are included into the sequence. A threshold of 500 milliseconds was used in Feher et al.'s \cite{mouse-dynamics} study. This was reduced to 300 milliseconds for capturing Dota 2 mouse features as it was found a 500 millisecond threshold created fewer and longer mouse movement sequences than expected. 

Each movement sequence consisted of three vectors:
\begin{itemize}
\item $\boldsymbol{t} = \{t_i\}^{n}_{i=1}$ - The game tick
\item $\boldsymbol{x} = \{x_i\}^{n}_{i=1}$ - The x coordinate sampled on game tick $t_i$
\item $\boldsymbol{y} = \{y_i\}^{n}_{i=1}$ - The y coordinate sampled on game tick $t_i$
\end{itemize}
The length $i$ is the same across the three vectors, but each MM action can have varying values of $i$. The vectors themselves are further processed to give the following list of basic movement features, based on the approach described by Gamboa et al. \cite{mouse-features}:
\begin{table}[H]
\renewcommand*{\arraystretch}{2.5}
\centering
\begin{tabular}{| c | c | c |}
\hline
& \textbf{Feature} & \textbf{Definition} \\ \hline
1 & Angle of movement & $\theta_i = arctan(\dfrac{\delta y_1}{\delta x_1}) + \sum\limits_{j=1}^{i} \delta \theta_j$ \\ \hline
2 & Curvature & $c = \dfrac{\delta\theta}{\delta s}$ \\ \hline
3 & Rate of change of curvature & $\Delta c = \dfrac{\delta c}{\delta s}$ \\ \hline
4 & Horizontal velocity & $V_x = \dfrac{\delta x}{\delta t}$ \\ \hline
5 & Vertical velocity & $V_y = \dfrac{\delta y}{\delta t}$ \\ \hline
6 & Velocity & $V = \sqrt{\delta V_{x}^{2} + \delta V_{y}^{2}}$ \\ \hline
7 & Acceleration & $V' = \dfrac{\delta V}{\delta t}$ \\ \hline
8 & Jerk & $V'' = \dfrac{\delta V'}{\delta t}$ \\ \hline
9 & Angular velocity & $w = \dfrac{\delta \theta_t}{\delta t}$ \\ \hline
\end{tabular}
\caption{Basic mouse movement features used in \cite{mouse-features}}
\label{tbl:mm-features}
\end{table}

Next, the basic features are extracted into the statistics: minimum, maximum, mean and standard deviation. This was done because each of the basic features are vectors of varying length. For example, features such as velocity and curvature have $n + 1$ data points compared to their derivative features. Taking statistics of these vectors eliminates the problem with varied length features, but causes some data to be lost in the conversion.

The combination of a mouse movement sequence followed by a command action defines the \textbf{level 3 actions}:
\begin{itemize}
\item Mouse movement sequence followed by an attack command (MMAC)
\item Mouse movement sequence followed by a move command (MMMC)
\item Mouse movement sequence followed by a spell cast command (MMSC)
\end{itemize}
To create the three level 3 actions, the parser listens for attack, move and spell cast commands. If a command is recorded, the current movement sequence is used as the sequence leading up to the command. As such, these three features are identical in the way they are recorded, but potentially record very different kinds of data and takes into account keyboard presses by the player. For example, the move command is sent much more often than the other two commands. The features of the level 3 actions are statistics of the features in Table \ref{tbl:mm-features} with two additional features:
\begin{itemize}
\item Game ticks to commands $t_n$ - the number of game ticks between the last two mouse positions
\item Distance to command - the distance travelled between the last two mouse positions
\begin{equation}
d_i = \sqrt{\delta x_{i}^2 + \delta y_{i}^2}
\end{equation} 
where $\delta x_i = x_{i+1} - x_i$ and $\delta y_i = y_{i+1} - y_i$
\end{itemize}

The total features of each level 3 action is shown in Table \ref{tbl-level3features}, giving the total number of features as 38, or 114 features if all three types are used together.
\begin{table}[H]
\renewcommand*{\arraystretch}{1.5}
\centering
\begin{tabular}{| c | c | c |}
\hline
\textbf{Property} & \textbf{Features} & \textbf{Number of features} \\ \hline
Angle of movement & \multirow{9}{6cm}{Minimum, maximum, mean, standard deviation} & \multirow{9}{*}{$4 \times 9  = 36$} \\ \cline{1-1}
Curvature & & \\ \cline{1-1}
Rate of change of curvature & & \\ \cline{1-1}
Horizontal velocity & & \\ \cline{1-1}
Vertical velocity & & \\ \cline{1-1}
Velocity & & \\ \cline{1-1}
Acceleration & & \\ \cline{1-1}
Jerk & & \\ \cline{1-1}
Angular velocity & & \\ \hline
Game ticks to command & \multirow{2}{*}{Single value} & \multirow{2}{*}{2} \\ \cline{1-1}
Distance to command & & \\ \hline
\end{tabular}
\caption{Final processed features for each level 3 action}
\label{tbl-level3features}
\end{table}

\subsection{Game statistic features}
Another feature that was added for the machine learning models are game-specific statistics, which generally indicate the performance of a player, rather than purely their behaviour. This was the most common features used in previous studies and so was included to see how useful they may be in this use case.

The statistics are:
\begin{itemize}
\item Kills
\item Assists
\item Deaths
\item Gold per minute
\item XP per minute
\item CS (creep score) per minute
\item Denies
\item Actions per minute
\item No. of move commands on target per minute
\item No. of move commands on position per minute
\item No. of attack commands on target per minute
\item No. of attack commands on position per minute
\item No. of spell cast commands on target per minute
\item No. of spell cast commands on position per minute
\item No. of spell cast commands with no target per minute
\item No. of hold position commands per minute
\end{itemize}
Most of these statistics are taken as per minute because the numbers can vary greatly depending on the length of the game, so scaling the statistics by minute will take this into account. The number of denies is not taken per minute as denies typically only happen during the laning portion of a game, which always happens regardless of the length of the game. Further, the absolute number of denies is relatively low, with some players getting 0 denies often depending on their role in the team.

The gold, XP and creep score statistics are mainly a measure of performance. They are typically strong indicators for match outcome, as previous studies have shown \cite{dota-mixed-rank-win, dota-kinkade, dota-pu-yang, dota-yang} as winning teams usually have higher gold and XP numbers compared to losing teams. In this regard, they are likely less useful compared to other statistics, but can still provide some behavioural information. The reason many Dota 2 statistics platform exist is for players to track their performance via numbers such as gold per minute and creep score per minute. Experienced players can typically reach higher numbers and players often have very similar performance across multiple matches. As such, it would be interesting to see how useful these statistics are compared to the other statistics. The other statistics on command counts are likely to be more reflective of player behaviour, as they are a measure of how a player would control their hero during a match. As mentioned in section \ref{sec:dota-control}, there are multiple methods a player can use to accomplish the same actions in the game. The action statistics such as attack commands on target and attack commands on position are an indication of the different methods of control.

The number of kills, deaths and assists (KDA) for a player are very obvious statistics for player performance, but are unlikely to produce strong results for player behaviour. This is because the KDA of players usually varies, even for the same player, depending on their personal performance and the performance of their team for a single match. Furthermore, it is not uncommon for different players to have similar KDA statistics as there is not a large range of values for the number of kills, deaths and assists, making the statistics unlikely to be very useful in player prediction. However, they are not omitted to see if they could potentially be useful, especially if they are used together with all the other statistics.

\subsection{Game itemisation features}
As it was seen from previous studies on using Dota 2 match data \cite{dota-gao, dota-eggert}, the items purchased by players is a strong indicator to the hero and role of a player. This is because different roles and heroes in the game will purchase different items to fit that role. Itemisation alone is not telling of who the player is, as multiple players playing the same hero are likely to buy similar items to each other. However, the same items can be placed in different inventory positions for different players and is typically a personal choice by a player. Moreover, many items in Dota 2 have active abilities - such as the \textit{Blink dagger}, which teleports the player's character a short distance - providing the player with additional abilities that may be cast. As such, the location an item occupies in a player's inventory reflects the player control settings and is an easy feature that can differentiate different players. It is also unlikely that a player changes their control settings often, especially in the short term, making patterns in itemisation likely to reflect long term player behaviour.

\imagefig{0.35\textwidth}{imgs/inventory.png}{Example of the inventory with different items in Dota 2. The letters next to the items indicate they can be activated by pressing that key on the keyboard, which is bound to the position in the inventory, and is item-agnostic.}{fig:inventory}

There are two areas of concern for this feature: when to sample for item positions and how to encode this information into numerical data. For the first issue, the item names and inventory positions can be easily extracted at any game tick from the match replays. The items were chosen to be sampled twice  once at the beginning of a match and once at the end. This prevents a lot of variance in the items recorded. Items sampled at the beginning of a match will typically not be affected by variables such as how well the team have been doing through the match. Items sampled at the end are less likely to be affected by similar variables, as the items players buy throughout the match will be dictated by the flow of the match and players will be more likely to have bought most items they need or want by the end of the match. 

The problem of encoding the item data is more complicated, the information of which items were purchased and which inventory position they occupied had to be represented as a numeric value. This was tricky, as just using a numeric values such as the item id implies a relation between the items that is false. For example, item 1 is the blink dagger and item 2 is the blades of attack. Using the item ids for encoding would imply that the blink dagger is 1 value less than the blades of attack where in reality it is illogical to rank them this way as they qualitatively very different - the blink dagger costs more gold and provides an active ability, while the blades of attack costs less gold but provides additional bonuses to the player. As such, a few different encoding methods are explored and all used to compare how they perform. 


\subsubsection{Feature hashing}
One method for encoding categorical features is to hash the features into a number of new features \cite{feature-hashing}. This provides a unique numeric value for each possible category, spread over a controllable number of features. This method is better than a simple numeric conversion in that there is a smaller range of values in the hash that alleviate the implied numeric relationship. However, it doesn't completely remove the issue and there are possibilities of hash collisions where two different items share the same hash, though it is unlikely as there only 281 different item names. 

\subsubsection{One-hot encoding}
One-hot encoding is a well known and widely used approach to encode categorical values. It works by encoding each possible category as a separate binary feature, with a value of 0 or 1 representing the existence of the original value. There is no concern with the numeric values misrepresenting the categorical values because each value creates its own feature. The issue with this encoding method is the number of values to encode. Ideally, each combination of item and inventory position is encoded as unique value, which gives about 1800 binary features (\textasciitilde{}300 items $\times$ 6 inventory locations). 

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\begin{figure}[H]
\centering
\begin{subfigure}{1\textwidth}
\centering
\begin{tabular}{| c | c | c | c | c | c |}
\hline
Slot 1 & Slot 2 & Slot 3 & Slot 4 & Slot 5 & Slot 6 \\ \hline
item\_tango & item\_blink & item\_empty & item\_empty & item\_phase\_boots & item\_manta \\ \hline
\end{tabular}
\caption{Raw item slots and names extracted from a match replay.}
\end{subfigure}

\vspace*{1cm}

\begin{subfigure}{1\textwidth}
\begin{tabular}{| L{2cm} | L{2cm} | L{2cm} | c | L{2cm} | L{2cm} | L{2cm} | c |}
\hline
Slot 1 item\_tango & Slot 2 item\_tango & Slot 3 item\_tango & ... & Slot 1 item\_blink & Slot 2 item\_blink & ... \\ \hline
1 & 0 & 0 & ... & 0 & 1 & ... \\ \hline
\end{tabular}
\caption{One-hot encoded items}
\end{subfigure}
\caption{Example of one-hot encoding inventory items.}
\end{figure}

Depending on the type of machine learning model used, the large number of features created from using one-hot encoding may not negatively affect the accuracy those models. However, it will be computationally expensive, both for storing the pre-processed data and for training the models. 


\subsubsection{Selective one-hot encoding}
To alleviate the issues of one-hot encoding generating too many features, the number of items used as features can be restricted heuristically based on domain-specific knowledge of how Dota 2 plays as a game. The restrictions do not cause large losses of information. On the contrary, more information may be gained as the restrictions are chosen based on game knowledge. The two restrictions are as follows:
\begin{enumerate}
\item \textbf{Starting items only} - Every player starts a match of Dota 2 with the same amount of starting gold. This small amount of gold severely limits the number of items that can be purchased at the start of every match, bringing the number of items down from over 300 to about 30. Further, items that players buy at other points of a match are influenced to a greater extent by factors other than behaviour, such as whether a team is winning or losing. These factors are less influential (though they still exist) at the start of a match. The characteristic of different players placing items in different inventory slots still apply to starting items as a player's keyboard shortcuts for each inventory slot do not change. By restricting the items to only starting items, less features are created which reduce the computational load and there is less external influence, while still retaining the characteristic of player behaviour based on item selection. In this case, items data was only extracted at the start of a match and not the end, as it would be unlikely any starting items were still in a player's inventory by the end of the match. 

\item \textbf{Boots only} - Another telling aspect of the player identity is the inventory slot they place their boots in. In Dota 2, almost all players and heroes will buy a boots item, as it increases their movement speed. There are 5 types of advanced boots and 1 base boots item, making only 6 items (or 36 features) when only taking into account boots. As boots are unlikely to be purchased at the start of a match and cannot be upgraded to the advanced types of boots, this data was only extracted once at the end of a match. 
\end{enumerate}

% Heuristic for which items we think are most useful and only encode those


\subsubsection{Item differences}
A simple solution without the need to use an encoding scheme is to look at the differences in the items between the two matches being compared. This feature is very simple to encode, creating exactly one feature for each inventory slot. The idea here is if the player inventory of two matches contain the same items on the same slots, it is likely that the two matches belong to the same player. This approach is good and straightforward, encoding the exact information about item differences between players. However, there is significant loss of information as well, for example details about which items are in the inventory. This method only works in the case of using two matches as input in order to compute the difference.

\subsection{Data collection}\label{sec:data-collection}
To download the replays, a combination of the OpenDota \cite{opendota} API and Valve's official API was used. The OpenDota API was used to fetch a list of players and their match history given a number of conditions. This allowed the replays downloaded to be controlled in a very specific way. For example, the game mode and hero id were specified to ensure all matches returned by the API are from the same player, in the same game mode and playing the same hero. This control is important to isolate additional variables that may affect player behaviour. The game mode and hero played are especially important. Different game modes have slightly different rules, for example turbo mode where gold and experience gain is doubled. Keeping the hero chosen the same is also very important, as heroes themselves represent different playstyles, which would have a large impact on item choices and possibly on player mouse dynamics. Different heroes would also be more likely to play in a different role, which affect statistics such as gold gained and last hits on creeps. 

\begin{figure}[H]
\centering
\begin{tikzpicture}[>=stealth']
% Locations
\def\ClientToServer{++(6,0)}
\def\ServerToClient{++(-7,0)}
\def\ValveToClient{++(-12,0)}
\def\Lifeline{++(0,-10)}

% Lifelines
\path (0,0) node[draw] (Client) {Client}
      (7,0) node[draw] (Server) {OpenDota API}
      (12,0) node[draw] (Valve)  {Valve API};
\draw (Client) -- \Lifeline (Server) -- \Lifeline (Valve) -- \Lifeline;

% Blocks
\path (Server)
      ++(0,-1) node (BeginHeroes) {} node[below right] {\texttt{\ /heroes}}
      ++(0,-1) node (EndHeroes)   {};
\filldraw[fill=blue!30] (BeginHeroes.west) rectangle (EndHeroes.east);

\path (Server)
      ++(0,-3.25) node (BeginPlayer) {} node[below right] {\texttt{\ /players}}
      ++(0,-1) node (EndPlayer) {};
\filldraw[fill=red!30] (BeginPlayer.west) rectangle (EndPlayer.east);

\path (Server)
      ++(0,-5.5) node (BeginMatches) {} node[below right] {\texttt{\ /matches}}
      ++(0,-1) node (EndMatches) {};
\filldraw[fill=green!30] (BeginMatches.west) rectangle (EndMatches.east);

\path (Valve)
      ++(0,-7.75) node (BeginReplays) {} node[below right] {\ Replay cluster}
      ++(0,-1) node (EndReplays) {};
\filldraw[fill=black!30] (BeginReplays.west) rectangle (EndReplays.east);


% Calls
\draw[->] (BeginHeroes)\ServerToClient -- node[above] {\texttt{hero\_id}} (BeginHeroes);
\draw[->] (EndHeroes) -- node[above] {List of player ids} \ServerToClient;

\draw[->] (BeginPlayer)\ServerToClient -- node[above] {\texttt{player\_id, gamemode, hero\_id}} (BeginPlayer);
\draw[->] (EndPlayer) -- node[above] {List of match ids} \ServerToClient;

\draw[->] (BeginMatches)\ServerToClient -- node[above] {\texttt{match\_id}} (BeginMatches);
\draw[->] (EndMatches) -- node[above] {Replay cluster and salt} \ServerToClient;

\draw[->] (BeginReplays)\ValveToClient -- node[above] {\texttt{cluster\_id, salt}} (BeginReplays);
\draw[->] (EndReplays) -- node[above] {\texttt{.dem} replay file} \ValveToClient;

\end{tikzpicture}
\caption{Sequence of API calls to download  replay files}
\label{fig:api-calls}
\end{figure}

Figure \ref{fig:api-calls} shows the sequence of API calls that lead to acquiring the replay file from Valve servers. Multiple calls to the OpenDota API are required as each call returns the data needed for the next. This whole process is streamlined as a Node.js script, which takes three parameters: hero id, number of players and number of games per player. Other parameters such as the game mode are defined in the script as they were kept constant for all the datasets. The list of constant parameters are as follows:
\begin{itemize}
\item \textbf{Game mode} - This was kept constant as the \textit{All Pick} game mode for all the datasets collected.
\item \textbf{Team} - Kept constant as \textit{Radiant}
\item \textbf{Hero id} - This was different for each dataset. In fact, the hero id defined the dataset as all other parameters were the same. 
\item \textbf{Date} - An addition added later in the project due to a large game changing patch. All matches downloaded were filtered to before the 18th of November 2018, to avoid changes from patch 7.20 affecting the existing work done. Note the patch only affected the itemisation features as it introduced a new slot for a specific items. This is a small issue, as everything else was patch-agnostic, but to keep the dataset consistent in the scope of this project, this filtered was implemented. 
\end{itemize}
Occasionally, the replay files were not found as they may have been deleted, or the cluster where they are stored was unreachable. This was not a major issue, as there were a large number of other players and replays that can be downloaded instead. The only implication was the number of replays downloaded may not be the exact number of replays specified (number of players $\times$ number of games per player) in the script. 

\subsection{Data processing}
There were two stages of data processing in the project: Parsing and pre-processing. The parsing stage refers to the extraction of raw data from match replays and pre-processing is the additional work to prepare the data for machine learning. 
\subsubsection{Replay parsing}
Match replays are run with functions from \texttt{clarity} and data extracted through the use of annotations in the Java code. The idea is that \texttt{clarity} runs through the full replay chronologically and provides annotations. If a certain event matches an annotation, the annotated function is called and the data extracted in that function. For example, the \texttt{@OnTickStart} annotation matches on every tick of a game and provides a \texttt{Context} object from which data can be extracted. 

\begin{lstlisting}[caption=Annotations and function definitions for parsing must adhere to \texttt{clarity}'s interface.]
@OnTickStart
public void onTickStart(Context ctx, boolean synthetic) {
    ...
}

@OnEntityCreated
public void onEntityCreated(Entity e) {
    ...
}
\end{lstlisting}

The parameters of annotated functions are fixed by \texttt{clarity}'s interface. A variety of annotations were used to extract the data, as they often required information from different objects and entities. To avoid monolithic functions on important annotations such as \texttt{OnTickStart}, each feature was extracted using separate parser classes which the main parser would call out to. The object-oriented nature of the parser classes allowed more features to be added easily. Moreover, each feature was represented by objects and parser classes to ensure the types and range of values did not interfere with other features. This also allowed for different number of data points for each feature in a match depending on the properties of the feature. For example, each level 3 action was extracted in a match for the forensic mouse movements, which gave thousands of level 3 actions for every match. On the other hand, there is only one set of game-specific features for each match.

\subsubsection{Further preprocessing}
The amount of pre-processing required differed for each feature. 
Jupyter notebooks were incredibly useful for this step, as the interactive nature allowed line by line changes and easy visualisation of the processed data. All pre-processing was done using the \texttt{pandas} library in Python, which allowed the processed dataframe to be directly used in machine learning. The library also provided useful functions for dealing handling erroneous data and general manipulation of the extracted data. 

Some features such as the game statistics required no preprocessing, while the itemisation feature required much more. For itemisation, all the encodings described were done during this pre-process step and created copies of the dataset with each encoding. For mouse movement features, the different types of movement were organised before they were passed into the machine learning models. All the features were stored in binary \texttt{Game} and \texttt{Pair} objects as a cache of the preprocessing step. Each of these objects contained the various features in their dataframe form that the \texttt{sklearn} classifiers can use for training and evaluation. 

\subsection{Machine learning overview}
The main goal of this project was to predict who the player is in a match of Dota 2. Naturally, this can be viewed as a classification problem to determine the player. However, there are small nuances in framing the problem, which led to two different questions to be formed:

\begin{enumerate}
\item \textit{Given a match of Dota 2, who is the player playing this hero?}
\item \textit{Given two matches of Dota 2, is it the same player playing on this hero?}
\end{enumerate}

As mentioned earlier, the chosen hero was kept constant to avoid additional variables affecting the extracted features. The two questions ask subtly different things, and it is important to understand their differences, especially when it comes to what the input and output to the machine learning problem should be. 

For the first question, the input is a single match of Dota 2 and the output is the identity of the player. This can be viewed as either a binary classification task by focussing on the identity of a single player, or a multi-class classification task by looking at a pool of players. This is an interesting problem, as it can show whether certain features, or combination of features, are able to identify one player's behaviour against other different players. It is the same problem that Liu et al. \cite{starcraft-identification} posed for the identification of Starcraft 2 players. Nevertheless, there is a fundamental limitation with this approach, and that is the generalisability of such a trained model on unseen players. In particular, a model trained to answer this question must be re-trained to identify a different player in the binary classification case, or re-trained on a different pool of players in the multi-class classification case. Either way, it cannot generalise to the broad playerbase of Dota 2 and predict on any player without re-training. This is what the second question attempts to answer. Instead of being restricted to work only on known players, a successful model that answers this question should be able to predict on any two matches. In theory, the models learn on the features of both matches and any pattern that may differentiate two different players. This can still be viewed as a binary classification task, with two matches from the same player in one class, and two matches from different players in the other. 

Further explanation on the details and results of each question are detailed in their respective sections (sections \ref{sec:game-classification} and \ref{sec:pair-classification}), along with discussion and analysis of why some features and models performed better than others 


\end{document}
