\documentclass[Report.tex]{subfiles}

\begin{document}

\section{Tools and technologies}
The project made use of a variety of tools and technologies. Many tools were crucial to the project, empowering one to easily download match replays and extract data from the replays.

\subsection{Existing software}

\subsubsection{Clarity}
\texttt{clarity} is an open source parser developed by Martin Schrodt for parsing Dota 2 replay files \cite{clarity}. It allowed the parsing and extraction of in-game data via match replay files. Because of the overwhelming amount of data that a single Dota 2 match contains, \texttt{clarity} does not simply output the game data. Instead, it provides an interface for programmers to extract the data or information needed. In many cases, the data is preprocessed and can be retrieved directly, such as the entity data of players. In other cases, the data is unprocessed and provided as an original protobuf object from the replay, where further processing is required. A large mix of \texttt{clarity}'s capabilities were employed in this project. Simple data was extracted to generate more features and more complex data fetched from utilising \texttt{clarity}'s interface. Though documentation was lacking, a large repository of example code and the source code for every protobuf object was available for reference. 

\texttt{clarity} is not the only parser available for Dota 2, there is also \texttt{smoke} (https://github.com/skadistats/smoke) for Python and \texttt{rapier} (https://github.com/odota/rapier) for Javascript. However, \texttt{clarity} is both faster and more complete (for example, \texttt{rapier} does not support in-game entities) than all the other options. It is also the only one that is still being actively developed to keep up with game updates, making it the best choice of parser for this project. 

Because \texttt{clarity} is written in Java, the parser written in the project also had to be in Java. Without this tool or the others like it, this project would not have been possible as the manual extraction of data from raw match replays would have been a large project in its own right. The parser also supports CS:GO (Counter-Strike: Global Offensive) match replays, allowing the results and work done in this project to be open for extension to include prediction for the CS:GO video game, though any Dota-specific approaches or features would not be easily adaptable. 

\subsubsection{OpenDOTA}
The OpenDota Project \cite{opendota} is an open source data platform which provides real time data for Dota 2 matches. Players can create an account on OpenDota, which synchronises with their in-game account to download and analyse replays of the player. This provides a player with diverse statistics useful for analysing performance over time. In addition to the performance statistics OpenDota provides players, the project also provides a web API for developers to fetch data from the platform. This API is useful for this project, as it provides an easy method to fetch a list of matches and players. The options that the API provides also help ensure the matches and players listed fall under certain categories and conditions, which were used to isolate many variables such as team (radiant or dire), hero and game mode. 

There are many other Dota 2 statistics platforms such as Dotabuff \cite{dotabuff} and datdota \cite{datdota}, however these platforms do not provide a programmable API to fetch data from as they cater to players. The details needed to download the exact replay of matches were also unavailable from these websites, which would prevent direct parsing of matches for data. Valve Corporation also has an official API for Dota 2, but contained a lot fewer features and less information. For some data, such as the binary file of match replays, the official API had to be used as it was the only source to download replays. For these reasons, the OpenDota API was used to fetch metadata on matches and players, with Valve's official API to download replays. 


\subsection{Technologies used}
Rather than use the same programming language for the whole project, a mix of different languages and technologies were used for different sections of the project based on their suitability in each area. This allowed each language to be leveraged to the best of their strengths, with the trade-off being the need to interface between them.
 
\subsubsection{Replay downloading}
Since the OpenDota API is a web API which returns JSON objects as responses, web based technologies such as Javascript was a strong choice, as it can easily make HTTP requests and natively parse the responses. 

The use of the API required several requests - detailed in section \ref{sec:data-collection} - to download a single replay, and it needed to be scaled to make thousands of calls to download a large dataset. The asynchronous nature of Javascript HTTP requests simplified and streamlined the downloading process without loss of extensibility. For example, the OpenDota API has a rate limit for free API calls (60 calls per minute) and it was trivial to wrap each request function with a limiter to adhere to the rate limit. Moreover, receiving native JSON responses from the API made filtering straightforward in Javascript, as the response attributes could be directly accessed. Finally, the problem with interfacing the Javascript code with the rest of the project was not a big issue. Replays had to be saved locally as a cache to prevent the need to fetch them over and over again anyway, so no additional step or interface was required between replay downloading and replay parsing. 

Consequently, Javascript was the chosen language to download Dota 2 match replays. 

\subsubsection{Replay parsing}
With \texttt{clarity} only available for Java, it was the chosen language for parsing match replays by default. The other parsers could also be used, but as stated above, they were relatively incomplete compared to \texttt{clarity}. Furthermore, using Java was not a disadvantageous restriction, as it was very suitable for this task. The static type system in Java forced the data extracted from match replays to conform to defined classes, ensuring a minimum level of data quality. The object-oriented nature of the language meant that the creation of features and parsers could follow an inheritance hierarchy, especially when features were related so new features are added easily without breaking or re-writing structural parts of code. Overall, there were no significant disadvantages to using Java for parsing, so it was used to take advantage of \texttt{clarity}'s completeness.


\subsubsection{Machine learning}
The \texttt{scikit-learn} library \cite{sklearn} is an open source machine learning library in Python which contains a vast number of functions and tools for machine learning algorithms. It interfaces with popular Python libraries such as NumPy and matplotlib, making it accessible and easy to use for machine learning. The library was used heavily to prevent the need to implement machine learning algorithms, such as a random forest classifier or logistic regression, by hand. Though there is some loss in the ability to customise cost functions and small details, \texttt{scikit-learn} provides a generic interface that gives fine grained control on most details of each machine learning algorithm. This allowed the project to focus on the extraction and processing of data from Dota 2 match replays, and the analysis of the various features, rather than being caught up in implementation of typical algorithms. The ease of use of the library also allowed repeatable experiments and multiple machine learning classifiers to be tested and compared.

To use \texttt{scikit-learn}, Python would be used for the machine learning section of this project. Pre-processing of parsed data was done in Python using the powerful data analysis library \texttt{pandas} \cite{pandas} so that no interfacing between pre-processing and machine learning would be required. The \texttt{pandas} dataframes are also built on NumPy interfaces, which allow them to be passed to \texttt{scikit-learn} library functions directly. The disadvantage is the lack of performance, especially when it comes to memory requirements for large datasets and dataframes. However, this does not affect the results or experimentation, as the performance of the models is far more important than the performance of the code, though it does limit the size of datasets for practical reasons.

\subsection{Testing and continuous integration}
The project contained a few different moving parts which could break at different places. In order to ensure the pipeline from parsing to machine learning went smoothly as additional features were added, Jenkins, a continuous integration framework was set up. The set up was fairly standard, with the build triggered on every commit to version control. Instead of using unit tests, the continuous integration runs the full parsing and machine learning pipeline on a few match replays. This was done to make sure data could be properly parsed, then processed and trained on without issues such as missing data, conversion errors or NaNs. 

\imagefig{1.0\textwidth}{imgs/travis.png}{Builds failing and succeeding in Jenkins, which help catch and fix issues when adding new features to the code.}{fig:ci}

The continuous integration was very useful to make sure parsed data could be transformed and that the preprocessing did not introduce erroneous data. This was important as multiple pre-processing steps and experiments ran with the same machine learning pipeline, so a change in the pipeline for one set of experiments could affect the other experiments, and the continuous integration would be able to catch it. 

\end{document}
