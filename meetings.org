* Report notes

** Parsing
- Parsed mouse movement following mouse movement dynamics paper
- As the paper used different 'level' of mouse movements with clicks, we take it as different actions taken by the player after a mouse movement (ATTACK, MOVE CHARACTER, CAST SPELL)
- AFAIK the 'iCusor' positions is relative to the screen of the player and we don't know how screen resolution of different players may affect/skew the cursor positions

** Machine learning
- Bias in ML data due to large proportion of negative training examples (eg 20\% positive, 80\% negative)
- This was 'solved' by using 'balanced' weight for the classifiers (inversely proportional to the number of training samples)
- Also 'solved' by undersampling the negative samples


1. Started with just mouse movements, looking at each row individually
- Made sure mouse movements from the same game are not in both training and testing data to avoid data leaking
- Classify the individual rows and get overall accuracy/precision/recall

2. Looked at classifying entire games by combining the outputs of the mouse movement features together
- First by averaging the probability of mouse movements to get one probability for each feature, then use simple voting to decide (This approach does not take into account any weights, if some features might be more useful than others)
- Next use a neural network to combine the probabilities so the different features can be weighted accordingly (This requires additional training, to train the neural net alongside each feature classifier)
- Maybe could try just combining all the features together and put all into neural network, rather than separate feature classifiers
- Combination of features and training per game greatly increases the accuracy, but precision/recall are hard to gauge due to many less data points for training/testing

3. Look at classifying pairs of games instead of a single game 
- We still got quite good results because we are only looking at a fixed subset of players
- Asking the question "Which player is it out of these 10,20,100?", relatively easy question to answer and not easily generalisable
- A more difficult question is, "Given these two games, are they the same player?" 
- Train on pairs of games that are either the same player or different players and then try to predict given two games

4. Needed to average the averages for mouse movement, which means we lose more information
- To lessen this, split the games into sections so less effect on average and we can also check if one section is better than another
- We can plot graphs to see if different sections are better for accuracy or not

* Week 1 - 18/09/18
First meeting to discuss project. Main idea of the project is to predict who the player is in a game of DOTA based on replay data. 

** Initial plan (technical)
Start with parsing mouse movement from replays using ~clarity~ or the python replay parser to get angles/curves. 
- http://www.ise.bgu.ac.il/faculty/liorr/clint1.pdf

Start with the same player on the same hero using binary classification - is the player playing this hero X or not?

Further game knowledge prediction features later and predict with features both independently and combined

Other ideas:
- Predicting strategies of players

- tracking statistics for pro players
- detect previously banned players
- anonymity in online games leading to antisocial behaviour due to lack of consequences > easily make new accounts to continue behaviour despite bans, especially in free to play games
- smurfing ranked games affecting players at lower skill level
- elo boosting and selling high ranked accounts (hard to detect)
- use case of 'stolen' accounts, where player behaviour suddenly changes to be very different
- 'identity theft' in online amateur tournaments

* Week 2 - 27/09/18
** Before meeting notes
DOER:
- need to add objective about identifying features to extract from replays?

Parser:


** TODO next week
 - [x] Auto replay download with OpenDOTA API
 - [x] Get player entity given steamID
 - [x] Parse player mouse movement data in ML friendly format
 - [] Process mouse movement data for ML based on identification paper
 - [] Run ML on dataset of mouse movement data

** Other notes
- Might need to get ethics approval

This project aims to predict players in the video game Dota 2 by analysing replays of players and running the data through a machine learning model. The purpose is to detect players using accounts for illegitimate reasons such as account selling. Publicly available data from Dota 2 players will be collected from the internet and processed. The research will be done in St Andrews and no interaction between the researcher and the players will take place.

- not interested in personal name, just interested in matching the account ids together
- not possible to obtain consent
- getting data from valve not opendota
- potential for identifiable names, but not interested, so will not store account names only ids
- say more about why, detect whether its the same person behind the account
- how long to keep the data, not going to store user id, analysed data kept indefinitely, anonimysed by removal of handle names

- steam API key = FFF909F2C1F90CFD66D568D188DBF371

* Week 3 - 04/10/18

** TODO week
- [x] Process mouse movement data for ML with categorised features
- [x] Setup ML pipeline
- [] Tune hyper parameters (CTT) with for precision/recall curve
- [x] Setup java jar and script for running full pipeline smoothly
- [] 

* Week 4 - 11/10/18
** TODO week
- [x] Use all mouse events in game rather than as individual data points for ML 
- [x] Use average/voting for mouse events - eg out of 100, how many are positive, if over 50 take as positive
- [x] Make sure to not use evaluation game in training
- [] Which percentage of mouse movements are more indicative (eg at beginning/end of match, during events/casts)
- [] Scripts for allowing easy data gathering when tuning hyper parameters
  - [x] Script getting steamid from opendota
  - [x] Get list of players automatically and download their games given hero id
  - [x] Don't download replays if error from server
  - [] CTT threshold as parameter in java
  - [] Script python classifier to give accuracy/precision/recall results
- [] Find good way to average all the moves in game (average as input or output?)
  - [] Investigate bagging/boosting/stacking as methods to combine classifiers (https://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning)
  - [] Investigate combining probabilities as a method to combine the movement classifiers
  - [] Investigate correlation of mouse movements features to output and look into reducing dimensionality (eg. PCA) REGULARISATION
  - [] Consider attack/move/cast as separate feature sets rather than a single "mouse movement" feature set

Notes: 
- the combination of classifiers will be important not only for the attack/move/cast mouse movements, but also when future features such as hero/item/skill selection is used
- Must also consider adding weights to the different features and how to learn/set the weights to give best results (experimental? or following some paper? Easiest to use another model/network)

** Links
- On Combining Classifiers https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=667881
- On Feature Combination for Multiclass Object Classification http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/PID953627_5937[0].pdf

- VotingClassifier https://stackoverflow.com/questions/45074579/votingclassifier-different-feature-sets

- Dr K Sirlantzis “Diversity in Multiple Classifier Systems”, University of Kent;www.ee.kent.ac.uk;
- F. Roli, Tutorial Fusion of Multiple Pattern Classifier”, University of Cagliari
- Robert P.W.Duin, “The Combining Classifier: to Train or Not to Train?”, ICPR 2002, Pattern Recognition Group, Faculty of Applied Sciences;
- L. Xu, A. Kryzak, C. V. Suen, “Methods of Combining Multiple Classifiers and Their Applications to Handwriting Recognition”, IEEE Transactions on Systems, Man Cybernet, 22(3), 1992, pp. 418-435. 
- J. Kittle, M. Hatef, R. Duin and J. Matas, “On Combining Classifiers”, IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(3), March 1998, pp. 226-239. 
- D. Tax, M. Breukelen, R. Duin, J. Kittle, “Combining Multiple Classifiers by Averaging or by Multiplying?”, Patter Recognition, 33(2000), pp. 1475-1485. 
- L. I. Kuncheva, “A Theoretical Study on Six Classifier Fusion Strategies”, IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(2), 2002, pp. 281-286.

** Paper on combining classifiers
https://ac.els-cdn.com/S0925231298000198/1-s2.0-S0925231298000198-main.pdf?_tid=8a144b8f-de12-4d06-8b16-3c5ebd8a8979&acdnat=1539689896_665ebbff81591bae8789fb402acf0728


*** Combine feature vectors into a single composite feature vector
Cons:
- Curse of dimensionality due to composition
- Difficulty in composition
- Redundancy (component feature vectors not independent of each other)

*** Combining classifiers
Reasons for combining classifiers
- Allows multiple classifiers to work on different features sets
- Allows the different features set to be used simultaneously

Combining classifiers can be seen as combination of multiple probability distributions - two frameworks to do combination:
1. aggregate classifier distributions into a single distribution to make the final decision
2. linear opinion pools where decision is made from a linear combination of classifier opinions

* Week 5 - 18/10/18
- Is that player the same person
- deal with bias towards 0s in dataset
- look at different features like boots/builds
- do they buy regen/mangoes (game based features)
- look if accuracy increases as we iteratively add more features
- looks at more general feature selection at the end
- at some point, use a network/model as the final classifier

11.00 on tuesday ILW


** TODO week
- [] CTT threshold as parameter
- [] Script python classifier to give accuracy/precision/recall for threshold tuning
- [x] List game specific features to add that could be useful, ie boots location or item choices 
- [] Parse the additional game specific features to get data and add into ML pipeline

- [x] Fix bias issue, either with undersampling or oversampling
- [] Refactor to allow easy addition of more features
- [x] Make sections/titles on report for flow/narrative
- [] Investigate correlation of mouse movements features to output and look into reducing dimensionality (eg. PCA) REGULARISATION

- [x] Sanity check with new data to make sure it still works
- [x] Increasing number of players should decrease accuracies
- [x] Create new ML by training pairs and take two games, and output if they are the same player or not
- [x] Investigate split game into segments before averaging to reduce data loss and also see if different segments are more indicative

Can I identify this player among a small group of players?
Given a game and an account id, is that game by the account id
- Does the game belong with in set of other games
- Then given a group of games from a period before, are they the same?
- Also which features are the most indicative
Key thing is not having to retrain


Fix model, given a game from a particular player, then given an account id, is the original game by the same player
- train on pairs of games, then when given one game, then another, is it the same player?
- No retraining, given a new game and account id, give probability two people are the same
- 

Possibly train on many many players, then given an game, is it a player we know

** Other features
- KDA
- Gold/XP per minute 
- Last hits/denies
- (Hero played)
- Action counts (Move position, move target, attack position, attack target, cast position, cast target, cast no target)

- Last hits at 10 minutes (last hit at different minute counts 5, 10, 15, 20, 25, 30)
- Denies at 10 minutes
- Spell casts of Q,W,E,R (likely specific to a hero)


Some features may only be adding noise to the system 

Note many of these features should be recorded relative to the duration of the game, eg actions per minute or gold per minute because the duration of the game can affect the numbers greatly.

* Week 7 - 02/11/18
- Exclude a player completely and see if it works
- See if increasing training examples makes it better
- Check if beginning/early game which is more indicative
- Look out for false negatives/positives which is better


** TODO week
- [] Keep evaluation/testing players separate from training to see if it still works
- [] See if continually increasing training samples increases accuracy
- [] Use split games to see which sections are better
- [] Add more features
- [] Draft report sections

https://colab.research.google.com/drive/1svVS1zKTJTabhPHQKxm8sr-31w8SQh0j#scrollTo=AjQFbe9PctSR


* Week 8

** TODO week
- [] Using graphics card?
- [] More features
- [] At what stage adding more data isn't helpful
- [] See if some heroes are better than ones for prediction
- [] Use hero as a priori
- [] Use 5 or 6 heroes (smurf specific heroes) broodmother, tinker, juggernaut



